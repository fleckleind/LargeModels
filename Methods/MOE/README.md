# MOE
[Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)

MOE: consists of a gating network, probabilistically selects among a number of expert networks, each of which is trained to specialize in a subset of the input space.

## Mathematics

